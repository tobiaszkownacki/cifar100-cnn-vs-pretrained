Number of Convolutional Layers,Number of Linear Layers in Classifier,Other Model Features,Optimizer and Parameters,Loss (train/test),Accuracy (train/test)
15,3,"batch normalization, 4096 neurons in the layer, channels = [64, 128, 256, 512, 1024] block_config = [2, 3, 4, 4, 3]",default AdamW lr = 1e-3,1.0802 / 1.8081,"68.54% / 55.94%"
15,3,"batch normalization, 4096 neurons in the layer, dropout in classifier, channels = [64, 128, 256, 512, 1024] block_config = [2, 3, 4, 4, 3]",default AdamW lr = 1e-3,1.2514 / 1.7330,"64.97% / 56.09%"
15,3,"batch normalization, 4096 neurons in the layer, dropout after maxpool in each layer, channels = [64, 128, 256, 512, 1024] block_config = [2, 3, 4, 4, 3]",default AdamW lr = 1e-3,1.3235 / 1.7725,"62.63% / 54.78%"
15,3,"batch normalization, 4096 neurons in the layer, dropout after maxpool in each layer, dropout after each layer in classifier, channels = [64, 128, 256, 512, 1024] block_config = [2, 3, 4, 4, 3]",default AdamW lr = 1e-3,1.0332 / 1.6159,"71.44% / 59.88%"
15,3,"batch normalization, 4096 neurons in the layer, dropout after maxpool in each layer, dropout after each layer in classifier, channels = [64, 128, 256, 512, 1024] block_config = [2, 2, 3, 3, 3]",default AdamW lr = 1e-3,0.9562 / 1.5482,"73.23% / 61.91%"
15,3,"batch normalization, 4096 neurons in the layer, dropout after maxpool in each layer, dropout after each layer in classifier, channels = [64, 128, 256, 512, 1024] block_config = [3, 3, 3, 3, 3]",default AdamW lr = 1e-3,1.0519 / 1.5423,"70.67% / 61.14%"
10,3,"batch normalization, maximum number of neurons in the layer is 64",AdamW lr = 1e-3 wd= 1e-2,0.7541 / 2.0298,"76.36% / 51.68%"
10,3,"batch normalization, maximum number of neurons in the layer is 64","Adam wd = 1e-2, lr = 1e-3",2.0850 / 2.2806,"46.45% / 41.68%"
10,3,"batch normalization, maximum number of neurons in the layer is 64",AdamW lr = 1.5e-2 wd = 1e-2,0.8730 / 2.0812,"73.13% / 48.80%"
10,3,"batch normalization, maximum number of neurons in the layer - 64, dropout p=0.2 after each pooling layer (3 times)",AdamW lr = 1e-3 wd= 1e-2,1.0248 / 2.0197,"68.8% / 50.22%"
10,3,"batch normalization, maximum number of neurons in the layer - 64, dropout p=0.1 after each pooling layer (3 times)",AdamW lr = 1e-3 wd= 1e-2,0.9339 / 2.1303,"71.29% / 48.90%"
10,3,"batch normalization, maximum number of neurons in the layer - 64, dropout p=0.25 after each pooling layer (3 times)",AdamW lr = 1e-3 wd= 1e-2,1.1761 / 1.9740,"64.86% / 50.00%"
10,3,"batch normalization, maximum number of neurons in the layer - 64, dropout p=0.5 after the 1st and 2nd fc layer",AdamW lr = 1e-3 wd= 1e-2,1.5229 / 1.8564,"57.12% / 50.54%"
10,3,"batch normalization, maximum number of neurons in the layer - 64, dropout p=0.4 after the 1st and 2nd fc layer",AdamW lr = 1e-3 wd= 1e-2,1.3940 / 1.7332,"60.26% / 53.56%"
10,3,"batch normalization, maximum number of neurons in the layer - 64, dropout p=0.4 after the 1st and 2nd fc layer and dropout p=0.2 after each pooling layer",AdamW lr = 1e-3 wd= 1e-2,1.7393 / 1.7714,"51.44% / 51.67%"
10,3,"batch normalization, maximum number of neurons in the layer - 512, slow growth from 16, then another layer with x2, dropout p=0.4 after the 1st and 2nd fc layer and dropout p=0.2 after each pooling layer",AdamW lr = 1e-3 wd= 1e-2,0.6413 / 1.6610,"79.70% / 60.82%"
10,3,"Changed pooling order compared to previous model v10. Now pooling is after the 3rd, 6th and 9th conv layer",AdamW lr = 1e-3 wd= 1e-2,0.4697 / 1.6095,"85.08% / 63.32%"
10,3,"batch normalization, number of neurons increases to 256. Pooling only twice after the 4th and 8th layer. No dropout",AdamW lr = 1e-3 wd= 1e-2,0.5946 / 2.2578,"81.39% / 51.53%"
10,3,"changed number of neurons in subsequent layers compared to the previous model. Dropout after each pooling and after each merged layer. In the merged layer, p = 0.5",AdamW lr = 1e-3 wd= 1e-2,0.5691 / 1.5424,"81.93% / 62.63%"
10,3,"increased maximum number of neurons compared to v13 to 1024",AdamW lr = 1e-3 wd= 1e-2,0.3915 / 1.4825,"87.18% / 66.30%"
10,3,"same as v14",AdamW lr = 1e-3 wd = 1.5e-2,0.4024 / 1.5607,"87.06% / 64.16%"
10,3,"same as v14",AdamW lr=1e-3 wd = 0.01,0.4362 / 1.5251,"85.82% / 64.5%"
5,3,"batch normalization, maximum number of neurons is 512, padding=1",default AdamW lr = 1e-3,0.2192 / 2.1221,"92.93% / 57.95%"
5,3,"maximum number of neurons 64, padding=1",default AdamW lr = 1e-3,1.8916 / 2.1671,"48.36% / 43.36%"
5,3,"batch normalization, 64 neurons in convolution, maximum number of neurons in fc 120",SGD lr = 1e-1,1.4912 / 2.3123,"58.34% / 42.15%"
5,3,"batch normalization, 64 neurons in convolution, maximum number of neurons in fc 120, dropout 0.4 after fc layers",default AdamW lr = 1e-3,2.2342 / 2.0362,"40.87% / 45.18%"
5,3,"batch normalization, 64 neurons in convolution, maximum 120 neurons in fc, dropout 0.2 after each pooling and fc dropout 0.5",default AdamW lr = 1e-3,2.7172 / 2.3081,"30.09% / 39.04%"
5,3,"batch normalization, number of neurons in convolution increases from 64 to 256 (doubles each layer) then decreases again to 64, 2 pooling layers, dropout 0.4 in fc layers",default AdamW lr = 1e-3,1.8229 / 1.7264,"49.14% / 52.41%"
5,3,"number of neurons in convolution doubles from 32 to 512, two poolings and dropout 0.3 in fc layers",AdamaX lr = 1e-3,2.3129 / 2.1493,"38.18% / 43.12%",link,link
5,3,"batch normalization, maximum number of neurons in convolution and fc is 512, two poolings, dropout 0.4 after the first fc layer",default AdamW lr = 1e-3,0.6400 / 1.4323,"80.15% / 62.97%"
10,3,"same as the best", "Adam lr = 1e-3, without weight regularization",0.3796 / 1.6073,"87.78% / 63.91%"
10,3,"same as the best", "Adam lr = 1e-3, wd = 1e-4",0.6888 / 1.5652,"79.27% / 61.62%"
10,3,"same as the best", Adam lr = 1e-3 wd = 1e-5,0.5801 / 1.4788,"81.53% / 63.57%"
10,3,"same as the best", "SGD lr = 1e-1 wd=0 momentum = 0.9", "0.5767 / 1.5716","81.81% / 61.8%"
10,3,"same as the best", "SGD lr = 1.5e-1 wd=0 momentum = 0.9",0.6176 / 1.4857,"80.68% / 63.33%"
10,3,"same as the best", "SGD lr = 1.5e1 wd = 1e-4 momentum = 0.9","0.6691 / 1.5279","79.55% / 61.97%"
10,3,"same as the best, batch size changed to 128", "RMSProp lr=1e-3 wd = 0, momentum = 9.9","0.8330 / 1.7079","75.37% / 61.38%"
10,3,"same as the best, batch size changed to 128", "RMSProp lr=1e-4 wd = 0, momentum = 9.9","0.5166 / 1.6039","83.38% / 63.63%"
10,3,"same as the best, batch size changed to 128", "RMSProp lr=1e-4 wd = 1e-4, momentum = 9.9","0.9392 / 1.4480","72.30% / 61.57%"

